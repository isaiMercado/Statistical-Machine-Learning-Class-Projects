{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.00462761  2.90291256  5.01626632  0.99679008]\n",
      " [ 3.95474279  2.2994538   4.12494947  0.99603359]\n",
      " [ 1.09522611  0.7609651   4.76219447  4.96005035]\n",
      " [ 0.95001702  0.65133341  3.87597882  3.9726729 ]\n",
      " [ 2.08236019  1.30394882  4.88863685  4.03992074]]\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "\"\"\"\n",
    "@INPUT:\n",
    "    R     : a matrix to be factorized, dimension N x M\n",
    "    P     : an initial matrix of dimension N x K\n",
    "    Q     : an initial matrix of dimension M x K\n",
    "    K     : the number of latent features\n",
    "    steps : the maximum number of steps to perform the optimisation\n",
    "    alpha : the learning rate\n",
    "    beta  : the regularization parameter\n",
    "@OUTPUT:\n",
    "    the final matrices P and Q\n",
    "\"\"\"\n",
    "\n",
    "def Beta_Gradient_Descent_Matrix_Factorization(Observed_Matrix, Users_Matrix, Items_Matrix, Features_Number, steps=5000, learning_rate=0.0002, beta=0.02):\n",
    "    \n",
    "    Items_Matrix = Items_Matrix.T\n",
    "    \n",
    "    for step in range(steps):\n",
    "        \n",
    "        for row in range(len(Observed_Matrix)):\n",
    "            for col in range(len(Observed_Matrix[row])):\n",
    "                if Observed_Matrix[row][col] > 0:\n",
    "                    prediction = numpy.dot(Users_Matrix[row,:],Items_Matrix[:,col])\n",
    "                    target = Observed_Matrix[row][col]\n",
    "                    local_error = target - prediction\n",
    "                    for feat in range(Features_Number):\n",
    "                        Users_Matrix[row][feat] = Users_Matrix[row][feat] + learning_rate * (2 * local_error * Items_Matrix[feat][col] - beta * Users_Matrix[row][feat])\n",
    "                        Items_Matrix[feat][col] = Items_Matrix[feat][col] + learning_rate * (2 * local_error * Users_Matrix[row][feat] - beta * Items_Matrix[feat][col])\n",
    "                        \n",
    "                        \n",
    "        eR = numpy.dot(Users_Matrix, Items_Matrix)\n",
    "        e = 0\n",
    "        for row in range(len(Observed_Matrix)):\n",
    "            for col in range(len(Observed_Matrix[row])):\n",
    "                if Observed_Matrix[row][col] > 0:\n",
    "                    e = e + pow(Observed_Matrix[row][col] - numpy.dot(Users_Matrix[row,:],Items_Matrix[:,col]), 2)\n",
    "                    for feat in range(Features_Number):\n",
    "                        e = e + (beta/2) * ( pow(Users_Matrix[row][feat],2) + pow(Items_Matrix[feat][col],2) )\n",
    "        if e < 0.001:\n",
    "            break\n",
    "    return Users_Matrix, Items_Matrix.T\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    Observed_Matrix = numpy.array([\n",
    "         [5,3,0,1],\n",
    "         [4,0,0,1],\n",
    "         [1,1,0,5],\n",
    "         [1,0,0,4],\n",
    "         [0,1,5,4],\n",
    "        ])\n",
    "\n",
    "    N = len(Observed_Matrix)\n",
    "    M = len(Observed_Matrix[0])\n",
    "    Features_Number = 2\n",
    "\n",
    "    Users_Matrix = numpy.random.rand(N,K)\n",
    "    Items_Matrix = numpy.random.rand(M,K)\n",
    "\n",
    "    Predicted_Users_Matrix, Predicted_Items_Matrix = Beta_Gradient_Descent_Matrix_Factorization(Observed_Matrix, Users_Matrix, Items_Matrix, Features_Number)\n",
    "    print(numpy.dot(Predicted_Users_Matrix, Predicted_Items_Matrix.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MoviesClassifier(object):\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        self.Users_Matrix = None\n",
    "        self.Items_Matrix = None\n",
    "        self.User_To_Index = dict()\n",
    "        self.Movie_To_Index = dict()\n",
    "    \n",
    "    \n",
    "    def Train(self, file_name):\n",
    "        \n",
    "        Observed_Matrix = \n",
    "        \n",
    "        N = len(Observed_Matrix)\n",
    "        M = len(Observed_Matrix[0])\n",
    "        \n",
    "        Features_Number = 2\n",
    "\n",
    "        self.Users_Matrix = numpy.random.rand(N,K)\n",
    "        self.Items_Matrix = numpy.random.rand(M,K)\n",
    "\n",
    "        self.Gradient_Descent_Matrix_Factorization(Observed_Matrix, Features_Number)\n",
    "        return\n",
    "        \n",
    "     \n",
    "    def Predictions(self, users_movies_vector):\n",
    "        predictions = list()\n",
    "        for entry in users_movies_vector:\n",
    "            user_index = self.User_To_Index[entry.userID]\n",
    "            movie_index = self.Movie_To_Index[entry.movieID]\n",
    "            user_row = self.Users_Matrix[user_index]\n",
    "            movie_row = self.Items_Matrix[movie_index]\n",
    "            prediction = self.Predict(user_row, movie_row.T)\n",
    "            predictions.append(str(entry.testID) + \", \" + str(prediction))\n",
    "            \n",
    "        open(\"output.csv\", \"w\").write(predictions)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def Predict(self, user_row, item_col):\n",
    "        prediction = numpy.dot(user_row,item_col)\n",
    "        return prediction\n",
    "        \n",
    "        \n",
    "    def Error_Function(target, prediction):\n",
    "        return pow(target - prediction, 2)\n",
    "\n",
    "\n",
    "    def Error_Partial_Derivative_Users(self, target, prediction, item):\n",
    "        return 2 * (target - prediction) * item\n",
    "\n",
    "\n",
    "    def Error_Partial_Derivative_Items(self, target, prediction, user):\n",
    "        return 2 * (target - prediction) * user\n",
    "\n",
    "\n",
    "    def Is_Observed(self, entry):\n",
    "        return entry > 0\n",
    "\n",
    "\n",
    "    def Gradient_Descent_Matrix_Factorization(self, Observed_Matrix, Features_Number, steps=5000, learning_rate=0.0002, beta=0.02):\n",
    "\n",
    "        Items_Matrix = self.Items_Matrix.T\n",
    "\n",
    "        for step in range(steps):\n",
    "\n",
    "            total_error = 0.0\n",
    "\n",
    "            for row in range(len(Observed_Matrix)):\n",
    "                for col in range(len(Observed_Matrix[row])):\n",
    "                    if self.Is_Observed(Observed_Matrix[row][col]) == True:\n",
    "\n",
    "                        target = Observed_Matrix[row][col]\n",
    "\n",
    "                        user_row = Users_Matrix[row,:]\n",
    "                        item_col = Items_Matrix[:,col]\n",
    "                        prediction = self.Predict(user_row,item_col)\n",
    "\n",
    "                        for feat in range(Features_Number):\n",
    "                            Users_Matrix[row][feat] = Users_Matrix[row][feat] + learning_rate * self.Error_Partial_Derivative_Users(target, prediction, Items_Matrix[feat][col])\n",
    "                            Items_Matrix[feat][col] = Items_Matrix[feat][col] + learning_rate * self.Error_Partial_Derivative_Items(target, prediction, Users_Matrix[row][feat])\n",
    "\n",
    "                        user_row = Users_Matrix[row,:]\n",
    "                        item_col = Items_Matrix[:,col]\n",
    "                        prediction = numpy.dot(user_row,item_col)\n",
    "\n",
    "                        total_error = total_error + self.Error_Function(target, prediction)\n",
    "\n",
    "            if total_error < 0.001:\n",
    "                break\n",
    "\n",
    "        self.Users_Matrix = self.Users_Matrix\n",
    "        self.Items_Matrix = self.Items_Matrix.T\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "movies_classifier = MoviesClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Unnamed: 0  userID  movieID  rating  date_day  date_month  date_year  \\\n",
      "654          77860    6785      904     4.5        22           7       2006   \n",
      "1851         77997    6785     3683     3.5         5           1       2006   \n",
      "3843         77933    6785     1954     4.0        15           1       2006   \n",
      "3935         77888    6785     1225     4.0         5           1       2006   \n",
      "9820         78049    6785     6539     3.5         5           1       2006   \n",
      "10101        78055    6785     6785     3.5         1           3       2008   \n",
      "13844        78070    6785     7325     4.0         5           1       2006   \n",
      "14029        77827    6785      101     5.0         6           1       2006   \n",
      "15348        77824    6785       52     3.5         7           1       2006   \n",
      "18679        77901    6785     1270     4.0         5           1       2006   \n",
      "21409        78099    6785    44191     3.5        14           7       2006   \n",
      "21924        77972    6785     2973     4.5         7           1       2006   \n",
      "26456        77946    6785     2290     4.0         7           1       2006   \n",
      "37637        78073    6785     8376     4.0         7           1       2006   \n",
      "45877        78059    6785     6881     3.5         5           1       2006   \n",
      "47801        77936    6785     2012     3.0         5           1       2006   \n",
      "48754        78024    6785     5051     4.0        25           7       2006   \n",
      "51222        78095    6785    39183     4.5        25           5       2008   \n",
      "51805        78100    6785    44204     4.5        24           3       2008   \n",
      "53732        77970    6785     2890     3.5         6           1       2006   \n",
      "56359        77838    6785      296     4.0         5           1       2006   \n",
      "57014        77882    6785     1203     4.5        18           3       2006   \n",
      "58728        77880    6785     1199     4.5         7           1       2006   \n",
      "62040        78008    6785     3969     2.5         6           1       2006   \n",
      "62461        77890    6785     1230     5.0         5           1       2006   \n",
      "65328        78022    6785     4993     4.0         6           1       2006   \n",
      "69190        78105    6785    46970     4.0        10          11       2006   \n",
      "71429        77983    6785     3207     3.5         8           8       2007   \n",
      "72141        78000    6785     3812     3.5         5           1       2006   \n",
      "77294        78075    6785     8636     4.0         6           1       2006   \n",
      "...            ...     ...      ...     ...       ...         ...        ...   \n",
      "680491       77850    6785      543     4.0         5           1       2006   \n",
      "680986       77884    6785     1210     4.0         5           1       2006   \n",
      "682753       77952    6785     2355     4.0        24           7       2006   \n",
      "683318       78040    6785     5989     4.0        26           7       2006   \n",
      "684080       78034    6785     5673     3.5         7           1       2006   \n",
      "684901       77894    6785     1245     4.0        24           9       2006   \n",
      "691879       78102    6785    45722     3.5        31          12       2006   \n",
      "693082       78051    6785     6584     4.5         7           1       2006   \n",
      "693649       78047    6785     6375     4.5         6           1       2006   \n",
      "697477       77984    6785     3247     3.5         9           1       2006   \n",
      "698349       77858    6785      858     4.5         5           1       2006   \n",
      "698577       78043    6785     6216     4.5         4           6       2007   \n",
      "700664       77829    6785      141     3.5         6           1       2006   \n",
      "704732       78017    6785     4623     3.5         6           1       2006   \n",
      "706085       77898    6785     1257     4.0        11           1       2006   \n",
      "719696       77851    6785      588     3.5         5           1       2006   \n",
      "729678       77835    6785      260     4.5         5           1       2006   \n",
      "729704       77875    6785     1147     5.0         5           1       2006   \n",
      "730400       77848    6785      500     3.0         5           1       2006   \n",
      "735094       77993    6785     3489     3.0         5           1       2006   \n",
      "737469       77918    6785     1617     3.5         5           1       2006   \n",
      "740431       77959    6785     2571     3.5         5           1       2006   \n",
      "740912       77996    6785     3671     4.0         7           1       2006   \n",
      "743599       78077    6785     8781     3.5        18           2       2006   \n",
      "745056       77857    6785      780     3.0         9           1       2006   \n",
      "747493       77905    6785     1307     4.0         6           1       2006   \n",
      "749561       78085    6785    30749     4.0        18           6       2007   \n",
      "756756       77935    6785     2011     2.0         5           1       2006   \n",
      "757772       77999    6785     3755     3.0        10           1       2006   \n",
      "759122       77941    6785     2081     3.5         7           1       2006   \n",
      "\n",
      "        date_hour  date_minute  date_second  \n",
      "654            14           44           10  \n",
      "1851            7           17           13  \n",
      "3843           11            0            5  \n",
      "3935            6           52           23  \n",
      "9820           12           20           45  \n",
      "10101          19           23           26  \n",
      "13844           7            8           11  \n",
      "14029           9           14           12  \n",
      "15348           6           21           41  \n",
      "18679           7           19           29  \n",
      "21409          12           33           18  \n",
      "21924           4           36           13  \n",
      "26456           6           21           58  \n",
      "37637           4           33           41  \n",
      "45877           7            7           47  \n",
      "47801          13            4           11  \n",
      "48754          12           49           36  \n",
      "51222           6           23           52  \n",
      "51805          19           54           46  \n",
      "53732           4            6           17  \n",
      "56359           7           12            2  \n",
      "57014          14            5            1  \n",
      "58728           4           35           44  \n",
      "62040          14            9           51  \n",
      "62461           6           53           13  \n",
      "65328           4            6            3  \n",
      "69190          17           17            3  \n",
      "71429           5           48           26  \n",
      "72141           7           14           57  \n",
      "77294           9           10           36  \n",
      "...           ...          ...          ...  \n",
      "680491         13            4           52  \n",
      "680986         12           51           13  \n",
      "682753         15           53           52  \n",
      "683318         14           13           13  \n",
      "684080          4           37           14  \n",
      "684901          2           48           23  \n",
      "691879          6           59           31  \n",
      "693082          6           21           54  \n",
      "693649         15           32           35  \n",
      "697477          2           48           49  \n",
      "698349          6           55            5  \n",
      "698577          5           38            8  \n",
      "700664          9           10           50  \n",
      "704732          3           19            5  \n",
      "706085         14           27            5  \n",
      "719696         13            3           32  \n",
      "729678          7           23           58  \n",
      "729704          6           57           50  \n",
      "730400         13            4           48  \n",
      "735094          7            6           34  \n",
      "737469          7           16           38  \n",
      "740431         12           55           29  \n",
      "740912          4           35            4  \n",
      "743599         11           26           54  \n",
      "745056         14           45            2  \n",
      "747493          4            7           38  \n",
      "749561         18           10           45  \n",
      "756756          6           53           17  \n",
      "757772          9           50           29  \n",
      "759122         16           58            9  \n",
      "\n",
      "[267 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "train_set = pandas.read_csv('user_ratedmovies_train.dat','\\t')\n",
    "print(train_set[train_set.userID == 6785])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movies_classifier.Train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       testID  userID  movieID\n",
      "0           0    6785     2599\n",
      "1           1   10783      339\n",
      "2           2   64642    58162\n",
      "3           3    6971      296\n",
      "4           4   48802     1387\n",
      "5           5   61805     5029\n",
      "6           6   26738     3729\n",
      "7           7   63198    45672\n",
      "8           8    5925      762\n",
      "9           9   57835     1411\n",
      "10         10   19066     3504\n",
      "11         11   52136    45728\n",
      "12         12   28645     2763\n",
      "13         13   13472      253\n",
      "14         14   24515    59784\n",
      "15         15   70100    44665\n",
      "16         16   28855     2788\n",
      "17         17   45478     7149\n",
      "18         18   35488     6942\n",
      "19         19   12554     1587\n",
      "20         20   24495     4232\n",
      "21         21     267     2966\n",
      "22         22   63375     8813\n",
      "23         23   57512     1617\n",
      "24         24   41154     1479\n",
      "25         25   19587     5292\n",
      "26         26    4549      515\n",
      "27         27   41027     8665\n",
      "28         28   35814     1569\n",
      "29         29   18367     3755\n",
      "...       ...     ...      ...\n",
      "84970   84970   32747     7300\n",
      "84971   84971   16949     4963\n",
      "84972   84972    6757     5017\n",
      "84973   84973   52004    34530\n",
      "84974   84974   36632     5417\n",
      "84975   84975   15722     4974\n",
      "84976   84976   18256     2716\n",
      "84977   84977   41258     6808\n",
      "84978   84978   56561     8581\n",
      "84979   84979   15816     3996\n",
      "84980   84980   13472     6155\n",
      "84981   84981   26964     2641\n",
      "84982   84982   60319      174\n",
      "84983   84983   23423     5669\n",
      "84984   84984    2218     3114\n",
      "84985   84985    6018     5447\n",
      "84986   84986   39861     3733\n",
      "84987   84987   52576     4967\n",
      "84988   84988   26674     2761\n",
      "84989   84989    1860     7225\n",
      "84990   84990   34101     6250\n",
      "84991   84991   62091      339\n",
      "84992   84992   69625    49272\n",
      "84993   84993    8041    38061\n",
      "84994   84994    6550     3355\n",
      "84995   84995   47249    33499\n",
      "84996   84996   41443     4558\n",
      "84997   84997    8423     1376\n",
      "84998   84998   63405      318\n",
      "84999   84999   47613    40819\n",
      "\n",
      "[85000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "unknown_set = pandas.read_csv('predictions.dat','\\t')\n",
    "print(unknown_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movies_classifier.Predict(unknown_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
